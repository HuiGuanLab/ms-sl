# The overwrite of datasets

## Table of Contents
* [TV show Retrieval](#TVR)
* [Activitynet Captions](#Activitynet-Captions)
* [Charades-STA](#Charades-STA)
* [Data Download](#Data-Download)
* [Reference](#Reference)

## TV show Retrieval
TV show Retrieval (TVR) [1] is a multimodal dataset originally for video corpus moment retrieval, where videos are paired with subtitles that are generated by automatic speech recognition. It contains 21.8K videos collected from 6 TV shows, and each video is associated with 5 natural language sentences that describe a specific moment in the video. Following [2,3], we utilize
17,435 videos with 87,175 moments for training and 2,179 videos
with 10,895 moments for testing.

|             |Train  | Test  |
| :---------: | :--: | :--: |
| TV show Retrieval | 17,435 videos, 87,175 sentences | 2,179 videos, 10,895 sentences|

## Activitynet Captions

ActivityNet Captions [4] is originally developed for dense video captioning task, and is now a popular dataset for single video moment retrieval. It contains around 20K videos from Youtube, and the average length of videos is the largest among the three datasets we used. On average, each video has around 3.7 moments with corresponding sentence descriptions.We use the popular data partition used in [2, 3].

|             |Train  | Test  |
| :---------: | :--: | :--: |
| Activitynet Captions | 9,043 videos, 33,721 sentences | 4,430 videos, 15,753 sentences|

## Charades-STA

Charades-STA [5] is a dataset for single video moment retrieval. It contains 6,670 videos with 16,128 sentence descriptions. Each video has around 2.4 moments with corresponding sentence descriptions on average. We utilize the official data partition for model training and evaluation.

|             |Train  | Test  |
| :---------: | :--: | :--: |
| Charades-STA | 5,538 videos, 12,408 sentences | 1,334 videos, 3,720 sentences|

## Data Download

<table>
        <tr align="center">
          <th>Dataset</th><th>Baidu pan</th><th>Aliyun</th>
        </tr>
        <tr>
            <td>TVR</td>
            <td rowspan="3" align="center"><a href="https://pan.baidu.com/s/1UNu67hXCbA6ZRnFVPVyJOA?pwd=8bh4">https://pan.baidu.com/s/1UNu67hXCbA6ZRnFVPVyJOA?pwd=8bh4</a></td>
            <td>http://8.210.46.84:8787/prvr/data/tvr.tar</td>
        </tr>
        <tr>
            <td>Activitynet</td>
            <td>http://8.210.46.84:8787/prvr/data/activitynet.tar</td>
        </tr>
        <tr>
            <td>Charades-STA</td>
            <td>http://8.210.46.84:8787/prvr/data/charades.tar</td>
        </tr>
 </table>
 
## Reference
[1] Jie Lei, Licheng Yu, Tamara L Berg, and Mohit Bansal. 2020. TVR: A large-scale dataset for video-subtitle moment retrieval. In European Conference on Computer Vision. 447–463.

[2] Bowen Zhang, Hexiang Hu, Joonseok Lee, Ming Zhao, Sheide Chammas, Vihan Jain,Eugene Ie,and Fei Sha. 2020. A hierarchical multi-modal encoder for moment localization in video corpus. arXiv preprint arXiv:2011.09046 (2020).

[3] Hao Zhang, Aixin Sun, Wei Jing, Guoshun Nan, Liangli Zhen, Joey Tianyi Zhou, and Rick Siow Mong Goh. 2021. Video corpus moment retrieval with contrastive learning. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval. 685–695.

[4] Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, and Juan Carlos Niebles. 2017. Dense-captioning events in videos. In Proceedings of the IEEE International Conference on Computer Vision. 706–715.

[5] Jiyang Gao, Chen Sun, Zhenheng Yang, and Ram Nevatia. 2017. Tall: Temporal activity localization via language query. In Proceedings of the IEEE International Conference on Computer Vision. 5267–5275.
